%作业3：短文本的Topic Modeling
%
%使用EM算法，求解此概率图模型的参数
%
%要求：
%1）写出目标函数，目标函数的下界
%2）EM算法推导出的迭代式子

%***
%
%Huaqiang Wang
%
%2016K8009929035
%
%***

We assume the word-word Co-occurrence Matrix is Symmetric Matrix. Which is,  word set {w} and word set {w'} are the same. 

From Probablistic Graph, we gain:


$$P(w_i,w'_j,z_k)=P(z_k)P(w_i|z_k)P(w'_j|z_k)$$

For this question, we need to maximize:

$$\prod_{i=1}^N \prod_{j=1}^N P(w_i,w'_j)^{n(w_i,w'_j)}$$

Which is equivalent to maximizing:

$$ln\left ( \prod_{i=1}^N \prod_{j=1}^N P(w_i,w'_j)^{n(w_i,w'_j)} \right )$$

$$=\left ( \sum_{i=1}^N \sum_{j=1}^N n(w_i,w'_j) ln P(w_i,w'_j) \right )$$

where:


$$P(w_i,w'_j)=\sum_{k=1}^{K}P(z_k)P(w_i|z_k)P(w'_j|z_k)$$

s.t.

$$\sum_{i=1}^NP(w_i|z_k)=1$$

$$\sum_{j=1}^NP(w'_j|z_k)=1$$

$$\sum_{k=1}^KP(z_k)=1$$

***

For EM algorithm, we need to maximize:

$$L(\theta)=\sum_{q=1}^{M}c_i ln(P(x_q|\theta))$$

$$=\sum_{q=1}^{M}c_i \sum_{k=1}^{K}ln(P(x_q,z_k|\theta))$$

$$=\sum_{q=1}^{M}c_i ln \left (\sum_{k=1}^{K} P(z_k|x_q,\theta ')\frac{P(z_k|x_q,\theta ')}{P(x_q|z_k,\theta ')}\right )$$

$$\geq \sum_{q=1}^{M}c_i \sum_{k=1}^{K} P(z_k|x_q,\theta ')ln \left (\frac{P(z_k|x_q,\theta ')}{P(x_q|z_k,\theta ')}\right )$$

$$=l(\theta|\theta')$$


In this question, $P(x_q|\theta)=P(w_{iq},w'_{jq}|\theta)$. 

Then the lower bound to be optimized is:

$$\sum_{i,j}^{M}c_{ij} \sum_{k=1}^{K} P(z_k|x_q,\theta ')ln \left ( P(x_q,z_k|\theta)\right )$$

$$=\sum_{i,j}^{M}c_{ij} \sum_{k=1}^{K} P(z_k|w_i, w'_j,\theta ')ln \left ( P(w_i, w'_j,z_k|\theta)\right )$$

$$=\sum_{i,j}^{M}c_{ij} \sum_{k=1}^{K} P(z_k|w_i, w'_j,\theta ')ln \left (P(z_k|\theta)P(w_i|z_k,\theta)P(w'_j|z_k,\theta)\right )$$

where 

$$\sum_{l=1}^K P(z_k|w_i, w'_j,\theta ')=1$$

$$\sum_{k=1}^{K} P(z_k)=1$$

$$\sum_{i=1}^N P(w_i|z_k)=1$$


Using Lagrange Multiplier Method:


Define:

$$Lag=\sum_{i,j}^{M}c_{ij} \sum_{k=1}^{K} P(z_k|w_i, w'_j,\theta ')ln \left (P(z_k|\theta) P(w_i|z_k,\theta)P(w'_j|z_k,\theta)\right )$$

$$+\sum_{k=1}^{K}\alpha_k\left ( 1- \sum_{i=1}^{N}P(w_i|z_k) \right )$$

$$+\beta\left ( 1- \sum_{i=1}^{K}P(z_k) \right )$$

Derive function Lag with respect to $\alpha_k$, $\beta$, we get euqation set:

$$\sum_{i=1}^{N}c_{ij}P(z_k|w_i, w'_j,\theta ') -\alpha_k P(w_i|z_k)P(z_k)=0$$

$$\sum_{i=1}^{N}\sum_{j=1}^{N}c_{ij}P(z_k|w_i, w'_j,\theta ') -\alpha_k P(z_k)=0$$

where $1\leq i,j \leq K$

The arguments need to be calculated are: P(w_i|z_k) and P(z_k). Let $Q(z_k)$ be $P(z_k|w_i, w'_j,\theta ')$

The result is:

$$P(w_i|z_k)=P(w'_j|z_k)=\frac{\sum_{j=1}^{N}c_{ij}Q(z_k)}{\sum_{i=1}^{N}\sum_{j=1}^{N}c_{ij}Q(z_k)}$$

$$P(z_k)=\frac{\sum_{i=1}^{N}\sum_{j=1}^{N}c_{ij}Q(z_k)}{\sum_{k=1}^{K}\sum_{i=1}^{N}\sum_{j=1}^{N}c_{ij}Q(z_k)}$$

And from the Probablistic Graph:

$$Q(z_k)=P(z_k|w_i, w'_j,\theta ')=\frac{P(z_k)P(w_i|z_k)P(w'_j|z_k)}{\sum_{k=1}^{K}P(w_i|z_k)P(w'_j|z_k)}$$

***

PS: if word set {w} and {w'} are not the same, the answer is:

$$P(w_i|z_k)=\frac{\sum_{j=1}^{N}c_{ij}Q(z_k)}{\sum_{i=1}^{N}\sum_{j=1}^{N}c_{ij}Q(z_k)}$$

$$P(w'_j|z_k)=\frac{\sum_{i=1}^{N}c_{ij}Q(z_k)}{\sum_{i=1}^{N}\sum_{j=1}^{N}c_{ij}Q(z_k)}$$

$$P(z_k)=\frac{\sum_{i=1}^{N}\sum_{j=1}^{N}c_{ij}Q(z_k)}{\sum_{k=1}^{K}\sum_{i=1}^{N}\sum_{j=1}^{N}c_{ij}Q(z_k)}$$

$$Q(z_k)=P(z_k|w_i, w'_j,\theta ')=\frac{P(z_k)P(w_i|z_k)P(w'_j|z_k)}{\sum_{k=1}^{K}P(w_i|z_k)P(w'_j|z_k)}$$


