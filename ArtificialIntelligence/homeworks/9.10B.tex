\documentclass{article} % For LaTeX2e
% \documentclass{UTF8}{ctexart}
% \usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{textcomp}  % \textcelsius
% \graphicspath{{./fig/}}
% \usepackage{tikz}

\title{Homework 1}
\usepackage{geometry}
\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}



\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}

\begin{document}


\maketitle
%
% \begin{abstract}
% empty
% \end{abstract}

%%------------------------------------------------------------------------
% 2016K8009929035_王华强_01B

\section{Homework 1B}

% 作业A：决策树-PPT 11页

%     形式化定义：条件信息熵

%     进行证明：信息增益一定大于等于0

% 作业B：决策树-PPT 12页

%     决策树：条件信息熵的应用

%     构造一个数据，每一个条件数据有自己的取值，

%     构造的数据要体现用greedy算出来的决策树不是一个最优的决策树
    

Build a data set, and use it to prove that decision tree built by greedy algorithm may not be the best desicion tree.

\begin{theorem}

    Decision tree built by greedy algorithm may not be the best desicion tree.

\end{theorem}

\begin{proof}

If there is no limitation for building the tree, all ways for generating the tree will finally lead to tree with the same information entropy. Therefore, we must introduce limitation to get different entropy.

We bulid the data set as following, where A, B are attributes and X is class.

$$
  \begin{matrix}
   ID & A & B & X \\
   1 & 0 & 0 & 0 \\
   2 & 0 & 0 & 0 \\
   3 & 0 & 1 & 0 \\
   4 & 0 & 1 & 1 \\
   5 & 1 & 0 & 1 \\
   6 & 1 & 0 & 1 \\
   7 & 1 & 1 & 1 \\
   8 & 1 & 1 & 0 \\
  \end{matrix}
$$

%noclass: log8
%A: (1/4)log(4)+(3/4)log(4/3)

When using greedy algorithm, the first division happened at attribute A, with a entropy reduction of log8-((1/4)log(4)+(3/4)log(4/3)).

While using attribute B to make the 1st node spilt, the entropy loss is log8-2((1/2)log(2)), which is less than the former one.

Then we can choose a number between the two losses, and make sure the tree will not grow when the entropy loss is less than the number.

In that case, the greedy algorithm will stop here. Yet non-greedy algorithm will not stop and spilt the new nodes, which will generate the less entropy in that dataset.

Therefore the data set above can prove the theorem.

\end{proof}
\end{document}
