\hypertarget{ux4ebaux5de5ux667aux80fdux57faux7840ux4f5cux4e1a-9.10}{%
\section{人工智能基础作业
9.10}\label{ux4ebaux5de5ux667aux80fdux57faux7840ux4f5cux4e1a-9.10}}

\begin{itemize}
\tightlist
\item
  王华强
\item
  2016K8009929035
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{ux8bc1ux660e-ux6761ux4ef6ux4fe1ux606fux71b5ux4fe1ux606fux589eux76ca0}{%
\subsection{证明:
条件信息熵信息增益\textgreater{}=0}\label{ux8bc1ux660e-ux6761ux4ef6ux4fe1ux606fux71b5ux4fe1ux606fux589eux76ca0}}

Sol:

From:

\[Gain(X,Y)=H(Y)-H(Y|X)\] \[H(Y)=-\sum_{j}^{}P(I_j)log(P(I_j))\]
\[H(Y|X)=\sum_{i}H(Y|x_i)P(x_i)\]

We gain:

\[Gain(X,Y)=H(Y)-H(Y|X)\]
\[=-\sum_{j}^{}P(I_j)log(P(I_j))-\sum_{i}H(Y|x_i)P(x_i)\]
\[=-\sum_{j}^{}P(I_j)log(P(I_j))+\sum_{i}(\sum_{j}^{}P(I_j|x_i)log(P(I_j|x_i))))P(x_i)\]
\[=-\sum_{j}^{}log(P(I_j))\sum_{i}P(I_j|x_i)P(x_i)+\sum_{i}P(x_i)\sum_{j}^{}P(I_j|x_i)log(P(I_j|x_i))\]
\[=\sum_{i,j}P(I_j|x_i)P(x_i)log(\frac{P(I_j|x_i)}{P(I_j)})\]
\[=\sum_{i,j}P(I_jx_i)log(\frac{P(I_jx_i)}{P(I_j)P(x_i)})\]
\[=\sum_{i,j}P(I_jx_i)-log(\frac{P(I_j)P(x_i)}{P(I_jx_i)})\]

\texttt{-log} is a convex function, then we can use Jensen unequation.

From Jensen unequation:

\[\sum_{i,j}P(I_jx_i)-log(\frac{P(I_j)P(x_i)}{P(I_jx_i)})\]
\[>=\sum_{i,j}-log(P(I_jx_i)\frac{P(I_j)P(x_i)}{P(I_jx_i)})\]
\[=\sum_{i,j}-log(P(I_j)P(x_i))\] \[=-log(1)=0\]

Therefore:

\[Gain(X,Y)=H(Y)-H(Y|X)>=0\]

Also, we may import KL divergence, and prove that KL divergence is
non-negative;

ref: https://blog.csdn.net/MathThinker/article/details/48375523

\hypertarget{ux6784ux9020ux8badux7ec3ux6570ux636e-ux4f7fux5f97ux4f7fux7528greedyux7b97ux6cd5ux8ba1ux7b97ux51faux7684ux51b3ux7b56ux6811ux4e0dux662fux6700ux4f18ux7684ux51b3ux7b56ux6811}{%
\subsection{构造训练数据,
使得使用Greedy算法计算出的决策树不是最优的决策树}\label{ux6784ux9020ux8badux7ec3ux6570ux636e-ux4f7fux5f97ux4f7fux7528greedyux7b97ux6cd5ux8ba1ux7b97ux51faux7684ux51b3ux7b56ux6811ux4e0dux662fux6700ux4f18ux7684ux51b3ux7b56ux6811}}

\begin{longtable}[]{@{}llll@{}}
\toprule
A & B & C & 标签\tabularnewline
\midrule
\endhead
0 & 1 & 1 & X\tabularnewline
0 & 1 & 1 & X\tabularnewline
0 & 1 & 0 & X\tabularnewline
0 & 1 & 0 & Y\tabularnewline
1 & 1 & 0 & Y\tabularnewline
1 & 0 & 1 & Y\tabularnewline
1 & 0 & 1 & Y\tabularnewline
1 & 1 & 0 & X\tabularnewline
\bottomrule
\end{longtable}

构造理念:

数据必须有无法区分项, 否则在所有数据都能用不同属性区分开的情况下,
必定能生成熵为0的决策树;

在这种情况下, 使用贪心算法完全没有问题;

因此必须构造干扰项:

使用第一层+第二层可以取得更低的信息熵(较好分类)
贪心算法使用第一层获取局部较大信息熵, 但是无法较好分类
