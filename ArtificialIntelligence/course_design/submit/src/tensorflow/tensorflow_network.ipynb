{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126208\n",
      "[[0, 1], [0, 0, 0, 3, 1, 3, 0, 8, 11, 7, 7, 7, 7, 7, 0, -8, -3, 7, -1, -4], [0, 8, 11]] [[1, 0], [0, 0, 1, 3, 1, 3, 0, 8, 13, 7, 7, 7, 7, 7, 0, -8, -5, 7, -1, -6], [0, 8, 13]] [[1, 0], [0, 0, 2, 3, 1, 3, 0, 8, 15, 7, 7, 7, 7, 7, 0, -8, -7, 7, -1, -8], [0, 8, 15]]\n",
      "epoch: 0 step: 5 acc: 0.8871094 loss: 0.42398387\n",
      "epoch: 0 step: 10 acc: 0.878125 loss: 0.4351472\n",
      "epoch: 0 step: 15 acc: 0.8625 loss: 0.45076266\n",
      "epoch: 0 step: 20 acc: 0.9238281 loss: 0.38943443\n",
      "epoch: 0 step: 25 acc: 0.9390625 loss: 0.37434834\n",
      "epoch: 0 step: 30 acc: 0.95585936 loss: 0.35740352\n",
      "epoch: 0 step: 35 acc: 0.9195312 loss: 0.39373127\n",
      "epoch: 0 step: 40 acc: 0.9652344 loss: 0.34802812\n",
      "epoch: 0 step: 45 acc: 0.9441406 loss: 0.36912194\n",
      "epoch: 0 step: 50 acc: 0.91328126 loss: 0.39998132\n",
      "epoch: 1 step: 5 acc: 0.92226565 loss: 0.39099693\n",
      "epoch: 1 step: 10 acc: 0.9453125 loss: 0.36795005\n",
      "epoch: 1 step: 15 acc: 0.9351562 loss: 0.37810627\n",
      "epoch: 1 step: 20 acc: 0.93125 loss: 0.38201252\n",
      "epoch: 1 step: 25 acc: 0.92265624 loss: 0.3906065\n",
      "epoch: 1 step: 30 acc: 0.946875 loss: 0.36638752\n",
      "epoch: 1 step: 35 acc: 0.9183594 loss: 0.39490643\n",
      "epoch: 1 step: 40 acc: 0.9585937 loss: 0.35466877\n",
      "epoch: 1 step: 45 acc: 0.9488281 loss: 0.36443433\n",
      "epoch: 1 step: 50 acc: 0.91679686 loss: 0.39646566\n",
      "epoch: 2 step: 5 acc: 0.9125 loss: 0.40076256\n",
      "epoch: 2 step: 10 acc: 0.9472656 loss: 0.36599794\n",
      "epoch: 2 step: 15 acc: 0.9464844 loss: 0.36677814\n",
      "epoch: 2 step: 20 acc: 0.92109376 loss: 0.3921688\n",
      "epoch: 2 step: 25 acc: 0.9398438 loss: 0.3734188\n",
      "epoch: 2 step: 30 acc: 0.94921875 loss: 0.36404377\n",
      "epoch: 2 step: 35 acc: 0.9441406 loss: 0.36912483\n",
      "epoch: 2 step: 40 acc: 0.9550781 loss: 0.3581844\n",
      "epoch: 2 step: 45 acc: 0.9183594 loss: 0.39490315\n",
      "epoch: 2 step: 50 acc: 0.9019531 loss: 0.41130942\n",
      "epoch: 3 step: 5 acc: 0.89023435 loss: 0.4230284\n",
      "epoch: 3 step: 10 acc: 0.93242186 loss: 0.38084078\n",
      "epoch: 3 step: 15 acc: 0.9273437 loss: 0.38591874\n",
      "epoch: 3 step: 20 acc: 0.9234375 loss: 0.38982502\n",
      "epoch: 3 step: 25 acc: 0.95234376 loss: 0.3609191\n",
      "epoch: 3 step: 30 acc: 0.9484375 loss: 0.364825\n",
      "epoch: 3 step: 35 acc: 0.9484375 loss: 0.3648255\n",
      "epoch: 3 step: 40 acc: 0.9183594 loss: 0.39492458\n",
      "epoch: 3 step: 45 acc: 0.91875 loss: 0.39451256\n",
      "epoch: 3 step: 50 acc: 0.9117187 loss: 0.4015438\n",
      "epoch: 4 step: 5 acc: 0.915625 loss: 0.39763755\n",
      "epoch: 4 step: 10 acc: 0.959375 loss: 0.35388762\n",
      "epoch: 4 step: 15 acc: 0.9207031 loss: 0.39255938\n",
      "epoch: 4 step: 20 acc: 0.9410156 loss: 0.3722469\n",
      "epoch: 4 step: 25 acc: 0.94453126 loss: 0.3687331\n",
      "epoch: 4 step: 30 acc: 0.93007815 loss: 0.38318443\n",
      "epoch: 4 step: 35 acc: 0.9453125 loss: 0.36796004\n",
      "epoch: 4 step: 40 acc: 0.9277344 loss: 0.38555068\n",
      "epoch: 4 step: 45 acc: 0.9203125 loss: 0.39295\n",
      "epoch: 4 step: 50 acc: 0.9003906 loss: 0.41287193\n",
      "epoch: 5 step: 5 acc: 0.9082031 loss: 0.40505943\n",
      "epoch: 5 step: 10 acc: 0.9707031 loss: 0.34255937\n",
      "epoch: 5 step: 15 acc: 0.9171875 loss: 0.39607507\n",
      "epoch: 5 step: 20 acc: 0.9507812 loss: 0.3624813\n",
      "epoch: 5 step: 25 acc: 0.9519531 loss: 0.36130944\n",
      "epoch: 5 step: 30 acc: 0.9566406 loss: 0.3566219\n",
      "epoch: 5 step: 35 acc: 0.9507812 loss: 0.36248234\n",
      "epoch: 5 step: 40 acc: 0.91796875 loss: 0.39529383\n",
      "epoch: 5 step: 45 acc: 0.9109375 loss: 0.40232506\n",
      "epoch: 5 step: 50 acc: 0.9019531 loss: 0.41130942\n",
      "epoch: 6 step: 5 acc: 0.915625 loss: 0.39763755\n",
      "epoch: 6 step: 10 acc: 0.87890625 loss: 0.4343562\n",
      "epoch: 6 step: 15 acc: 0.93007815 loss: 0.38318437\n",
      "epoch: 6 step: 20 acc: 0.9488281 loss: 0.36443442\n",
      "epoch: 6 step: 25 acc: 0.9476563 loss: 0.36560628\n",
      "epoch: 6 step: 30 acc: 0.9566406 loss: 0.35662186\n",
      "epoch: 6 step: 35 acc: 0.92929685 loss: 0.38396567\n",
      "epoch: 6 step: 40 acc: 0.92265624 loss: 0.39060634\n",
      "epoch: 6 step: 45 acc: 0.9238281 loss: 0.3894344\n",
      "epoch: 6 step: 50 acc: 0.8972656 loss: 0.41599688\n",
      "epoch: 7 step: 5 acc: 0.8703125 loss: 0.44295007\n",
      "epoch: 7 step: 10 acc: 0.8582031 loss: 0.45505944\n",
      "epoch: 7 step: 15 acc: 0.9242188 loss: 0.38904375\n",
      "epoch: 7 step: 20 acc: 0.9433594 loss: 0.36990315\n",
      "epoch: 7 step: 25 acc: 0.9589844 loss: 0.35427815\n",
      "epoch: 7 step: 30 acc: 0.9261719 loss: 0.38709062\n",
      "epoch: 7 step: 35 acc: 0.959375 loss: 0.35388753\n",
      "epoch: 7 step: 40 acc: 0.9359375 loss: 0.37732506\n",
      "epoch: 7 step: 45 acc: 0.9195312 loss: 0.39373127\n",
      "epoch: 7 step: 50 acc: 0.9160156 loss: 0.39724693\n",
      "epoch: 8 step: 5 acc: 0.9152344 loss: 0.39802822\n",
      "epoch: 8 step: 10 acc: 0.93671876 loss: 0.37654376\n",
      "epoch: 8 step: 15 acc: 0.93007815 loss: 0.38318437\n",
      "epoch: 8 step: 20 acc: 0.9183594 loss: 0.39490324\n",
      "epoch: 8 step: 25 acc: 0.9484375 loss: 0.364825\n",
      "epoch: 8 step: 30 acc: 0.921875 loss: 0.39138755\n",
      "epoch: 8 step: 35 acc: 0.95703125 loss: 0.35623127\n",
      "epoch: 8 step: 40 acc: 0.9488281 loss: 0.3644344\n",
      "epoch: 8 step: 45 acc: 0.9164063 loss: 0.3968563\n",
      "epoch: 8 step: 50 acc: 0.91015625 loss: 0.4031063\n",
      "epoch: 9 step: 5 acc: 0.953125 loss: 0.36013785\n",
      "epoch: 9 step: 10 acc: 0.9484375 loss: 0.364825\n",
      "epoch: 9 step: 15 acc: 0.91484374 loss: 0.3984188\n",
      "epoch: 9 step: 20 acc: 0.9359375 loss: 0.37732503\n",
      "epoch: 9 step: 25 acc: 0.94570315 loss: 0.36755937\n",
      "epoch: 9 step: 30 acc: 0.93671876 loss: 0.37654474\n",
      "epoch: 9 step: 35 acc: 0.9574219 loss: 0.35584062\n",
      "epoch: 9 step: 40 acc: 0.91796875 loss: 0.3952938\n",
      "epoch: 9 step: 45 acc: 0.90351564 loss: 0.40974694\n",
      "epoch: 9 step: 50 acc: 0.89375 loss: 0.41951266\n",
      "epoch: 10 step: 5 acc: 0.93710935 loss: 0.37615323\n",
      "epoch: 10 step: 10 acc: 0.9296875 loss: 0.38357502\n",
      "epoch: 10 step: 15 acc: 0.91914064 loss: 0.3941219\n",
      "epoch: 10 step: 20 acc: 0.94921875 loss: 0.3640439\n",
      "epoch: 10 step: 25 acc: 0.94921875 loss: 0.36404377\n",
      "epoch: 10 step: 30 acc: 0.96367186 loss: 0.34959093\n",
      "epoch: 10 step: 35 acc: 0.9195312 loss: 0.3937313\n",
      "epoch: 10 step: 40 acc: 0.9128906 loss: 0.40037194\n",
      "epoch: 10 step: 45 acc: 0.9109375 loss: 0.40232506\n",
      "epoch: 10 step: 50 acc: 0.90703124 loss: 0.4062313\n",
      "epoch: 11 step: 5 acc: 0.95234376 loss: 0.36091882\n",
      "epoch: 11 step: 10 acc: 0.91914064 loss: 0.39412192\n",
      "epoch: 11 step: 15 acc: 0.928125 loss: 0.38513756\n",
      "epoch: 11 step: 20 acc: 0.9398438 loss: 0.37341937\n",
      "epoch: 11 step: 25 acc: 0.9328125 loss: 0.38045004\n",
      "epoch: 11 step: 30 acc: 0.9429687 loss: 0.37029645\n",
      "epoch: 11 step: 35 acc: 0.92460936 loss: 0.3886618\n",
      "epoch: 11 step: 40 acc: 0.92226565 loss: 0.39099693\n",
      "epoch: 11 step: 45 acc: 0.9 loss: 0.41326255\n",
      "epoch: 11 step: 50 acc: 0.9128906 loss: 0.40037194\n",
      "epoch: 12 step: 5 acc: 0.9699219 loss: 0.34334064\n",
      "epoch: 12 step: 10 acc: 0.91015625 loss: 0.4031063\n",
      "epoch: 12 step: 15 acc: 0.94023436 loss: 0.3730282\n",
      "epoch: 12 step: 20 acc: 0.94804686 loss: 0.36521563\n",
      "epoch: 12 step: 25 acc: 0.9550781 loss: 0.3581844\n",
      "epoch: 12 step: 30 acc: 0.9507812 loss: 0.36248186\n",
      "epoch: 12 step: 35 acc: 0.92226565 loss: 0.39099693\n",
      "epoch: 12 step: 40 acc: 0.9085938 loss: 0.4046688\n",
      "epoch: 12 step: 45 acc: 0.88984376 loss: 0.42341882\n",
      "epoch: 12 step: 50 acc: 0.9160156 loss: 0.39724693\n",
      "epoch: 13 step: 5 acc: 0.9230469 loss: 0.39021564\n",
      "epoch: 13 step: 10 acc: 0.93359375 loss: 0.3796688\n",
      "epoch: 13 step: 15 acc: 0.9484375 loss: 0.36482504\n",
      "epoch: 13 step: 20 acc: 0.95234376 loss: 0.36091882\n",
      "epoch: 13 step: 25 acc: 0.96054685 loss: 0.35271558\n",
      "epoch: 13 step: 30 acc: 0.9265625 loss: 0.3867\n",
      "epoch: 13 step: 35 acc: 0.9265625 loss: 0.38670003\n",
      "epoch: 13 step: 40 acc: 0.91914064 loss: 0.39412194\n",
      "epoch: 13 step: 45 acc: 0.9 loss: 0.41326255\n",
      "epoch: 13 step: 50 acc: 0.8761719 loss: 0.4370907\n",
      "epoch: 14 step: 5 acc: 0.85664064 loss: 0.45662194\n",
      "epoch: 14 step: 10 acc: 0.92109376 loss: 0.39216876\n",
      "epoch: 14 step: 15 acc: 0.94804686 loss: 0.36521563\n",
      "epoch: 14 step: 20 acc: 0.95820314 loss: 0.3550594\n",
      "epoch: 14 step: 25 acc: 0.9328125 loss: 0.38045\n",
      "epoch: 14 step: 30 acc: 0.95351565 loss: 0.3597469\n",
      "epoch: 14 step: 35 acc: 0.93359375 loss: 0.3796688\n",
      "epoch: 14 step: 40 acc: 0.91328126 loss: 0.39998126\n",
      "epoch: 14 step: 45 acc: 0.9121094 loss: 0.40115324\n",
      "epoch: 14 step: 50 acc: 0.8964844 loss: 0.41677827\n",
      "epoch: 15 step: 5 acc: 0.9386719 loss: 0.37459064\n",
      "epoch: 15 step: 10 acc: 0.9277344 loss: 0.38552815\n",
      "epoch: 15 step: 15 acc: 0.9199219 loss: 0.39334068\n",
      "epoch: 15 step: 20 acc: 0.9496094 loss: 0.36365312\n",
      "epoch: 15 step: 25 acc: 0.92265624 loss: 0.3906063\n",
      "epoch: 15 step: 30 acc: 0.95585936 loss: 0.35740313\n",
      "epoch: 15 step: 35 acc: 0.95234376 loss: 0.36091876\n",
      "epoch: 15 step: 40 acc: 0.9152344 loss: 0.3980282\n",
      "epoch: 15 step: 45 acc: 0.9117187 loss: 0.4015438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 step: 50 acc: 0.9554688 loss: 0.35779393\n",
      "epoch: 16 step: 5 acc: 0.94257814 loss: 0.3706844\n",
      "epoch: 16 step: 10 acc: 0.9171875 loss: 0.39607507\n",
      "epoch: 16 step: 15 acc: 0.9355469 loss: 0.37771565\n",
      "epoch: 16 step: 20 acc: 0.9410156 loss: 0.3722469\n",
      "epoch: 16 step: 25 acc: 0.9332031 loss: 0.3800599\n",
      "epoch: 16 step: 30 acc: 0.96015626 loss: 0.35310626\n",
      "epoch: 16 step: 35 acc: 0.9230469 loss: 0.39021564\n",
      "epoch: 16 step: 40 acc: 0.9015625 loss: 0.41170007\n",
      "epoch: 16 step: 45 acc: 0.89921874 loss: 0.41404384\n",
      "epoch: 16 step: 50 acc: 0.9398438 loss: 0.37341878\n",
      "epoch: 17 step: 5 acc: 0.93007815 loss: 0.38318437\n",
      "epoch: 17 step: 10 acc: 0.91484374 loss: 0.3984188\n",
      "epoch: 17 step: 15 acc: 0.94492185 loss: 0.36834073\n",
      "epoch: 17 step: 20 acc: 0.9488281 loss: 0.3644344\n",
      "epoch: 17 step: 25 acc: 0.9597656 loss: 0.35349712\n",
      "epoch: 17 step: 30 acc: 0.9230469 loss: 0.39021564\n",
      "epoch: 17 step: 35 acc: 0.9152344 loss: 0.3980282\n",
      "epoch: 17 step: 40 acc: 0.90546876 loss: 0.40779382\n",
      "epoch: 17 step: 45 acc: 0.91328126 loss: 0.39998132\n",
      "epoch: 17 step: 50 acc: 0.95039064 loss: 0.36287194\n",
      "epoch: 18 step: 5 acc: 0.91679686 loss: 0.3964657\n",
      "epoch: 18 step: 10 acc: 0.921875 loss: 0.39138755\n",
      "epoch: 18 step: 15 acc: 0.940625 loss: 0.37263784\n",
      "epoch: 18 step: 20 acc: 0.9328125 loss: 0.38045\n",
      "epoch: 18 step: 25 acc: 0.9390625 loss: 0.3742011\n",
      "epoch: 18 step: 30 acc: 0.925 loss: 0.3882671\n",
      "epoch: 18 step: 35 acc: 0.92148435 loss: 0.39177817\n",
      "epoch: 18 step: 40 acc: 0.89921874 loss: 0.4140438\n",
      "epoch: 18 step: 45 acc: 0.91914064 loss: 0.3941219\n",
      "epoch: 18 step: 50 acc: 0.9675781 loss: 0.34568438\n",
      "epoch: 19 step: 5 acc: 0.903125 loss: 0.41013756\n",
      "epoch: 19 step: 10 acc: 0.9421875 loss: 0.37107506\n",
      "epoch: 19 step: 15 acc: 0.94570315 loss: 0.36755964\n",
      "epoch: 19 step: 20 acc: 0.95 loss: 0.3632625\n",
      "epoch: 19 step: 25 acc: 0.9519531 loss: 0.36131078\n",
      "epoch: 19 step: 30 acc: 0.9285156 loss: 0.38474768\n",
      "epoch: 19 step: 35 acc: 0.9097656 loss: 0.40349692\n",
      "epoch: 19 step: 40 acc: 0.8804687 loss: 0.4327938\n",
      "epoch: 19 step: 45 acc: 0.9175781 loss: 0.39568442\n",
      "epoch: 19 step: 50 acc: 0.94804686 loss: 0.36521563\n",
      "epoch: 20 step: 5 acc: 0.93476564 loss: 0.37849694\n",
      "epoch: 20 step: 10 acc: 0.9429687 loss: 0.3702938\n",
      "epoch: 20 step: 15 acc: 0.9554688 loss: 0.35779378\n",
      "epoch: 20 step: 20 acc: 0.9578125 loss: 0.35545\n",
      "epoch: 20 step: 25 acc: 0.92265624 loss: 0.39060628\n",
      "epoch: 20 step: 30 acc: 0.9265625 loss: 0.3867\n",
      "epoch: 20 step: 35 acc: 0.92109376 loss: 0.39216882\n",
      "epoch: 20 step: 40 acc: 0.8964844 loss: 0.41677818\n",
      "epoch: 20 step: 45 acc: 0.87460935 loss: 0.43865317\n",
      "epoch: 20 step: 50 acc: 0.8605469 loss: 0.4527157\n",
      "epoch: 21 step: 5 acc: 0.92265624 loss: 0.39060625\n",
      "epoch: 21 step: 10 acc: 0.9476563 loss: 0.36560625\n",
      "epoch: 21 step: 15 acc: 0.95625 loss: 0.35701245\n",
      "epoch: 21 step: 20 acc: 0.94023436 loss: 0.3730281\n",
      "epoch: 21 step: 25 acc: 0.94921875 loss: 0.3640438\n",
      "epoch: 21 step: 30 acc: 0.9308594 loss: 0.38240317\n",
      "epoch: 21 step: 35 acc: 0.91875 loss: 0.3945125\n",
      "epoch: 21 step: 40 acc: 0.909375 loss: 0.40388757\n",
      "epoch: 21 step: 45 acc: 0.8921875 loss: 0.4210751\n",
      "epoch: 21 step: 50 acc: 0.93789065 loss: 0.37537187\n",
      "epoch: 22 step: 5 acc: 0.92695314 loss: 0.3863094\n",
      "epoch: 22 step: 10 acc: 0.9277344 loss: 0.38552818\n",
      "epoch: 22 step: 15 acc: 0.9484375 loss: 0.364825\n",
      "epoch: 22 step: 20 acc: 0.91367185 loss: 0.39959067\n",
      "epoch: 22 step: 25 acc: 0.9597656 loss: 0.35349688\n",
      "epoch: 22 step: 30 acc: 0.9546875 loss: 0.35857502\n",
      "epoch: 22 step: 35 acc: 0.9125 loss: 0.40076256\n",
      "epoch: 22 step: 40 acc: 0.925 loss: 0.38826254\n",
      "epoch: 22 step: 45 acc: 0.95703125 loss: 0.35623136\n",
      "epoch: 22 step: 50 acc: 0.9394531 loss: 0.3738094\n",
      "epoch: 23 step: 5 acc: 0.91679686 loss: 0.39646566\n",
      "epoch: 23 step: 10 acc: 0.9375 loss: 0.37576252\n",
      "epoch: 23 step: 15 acc: 0.93476564 loss: 0.37849692\n",
      "epoch: 23 step: 20 acc: 0.9273437 loss: 0.38591912\n",
      "epoch: 23 step: 25 acc: 0.9546875 loss: 0.35857505\n",
      "epoch: 23 step: 30 acc: 0.9253906 loss: 0.3878719\n",
      "epoch: 23 step: 35 acc: 0.90234375 loss: 0.4109188\n",
      "epoch: 23 step: 40 acc: 0.89882815 loss: 0.4144345\n",
      "epoch: 23 step: 45 acc: 0.9359375 loss: 0.377325\n",
      "epoch: 23 step: 50 acc: 0.93242186 loss: 0.38084063\n",
      "epoch: 24 step: 5 acc: 0.9074219 loss: 0.4058407\n",
      "epoch: 24 step: 10 acc: 0.9433594 loss: 0.36990315\n",
      "epoch: 24 step: 15 acc: 0.9519531 loss: 0.3613094\n",
      "epoch: 24 step: 20 acc: 0.9609375 loss: 0.3523252\n",
      "epoch: 24 step: 25 acc: 0.9308594 loss: 0.38240314\n",
      "epoch: 24 step: 30 acc: 0.91367185 loss: 0.39959067\n",
      "epoch: 24 step: 35 acc: 0.9019531 loss: 0.41130942\n",
      "epoch: 24 step: 40 acc: 0.9117187 loss: 0.4015438\n",
      "epoch: 24 step: 45 acc: 0.94804686 loss: 0.36521572\n",
      "epoch: 24 step: 50 acc: 0.91796875 loss: 0.3952938\n",
      "epoch: 25 step: 5 acc: 0.91875 loss: 0.39451256\n",
      "epoch: 25 step: 10 acc: 0.94023436 loss: 0.37302834\n",
      "epoch: 25 step: 15 acc: 0.93046874 loss: 0.38279375\n",
      "epoch: 25 step: 20 acc: 0.934375 loss: 0.37888816\n",
      "epoch: 25 step: 25 acc: 0.92148435 loss: 0.39178097\n",
      "epoch: 25 step: 30 acc: 0.9203125 loss: 0.39295006\n",
      "epoch: 25 step: 35 acc: 0.89882815 loss: 0.41443443\n",
      "epoch: 25 step: 40 acc: 0.92148435 loss: 0.39177814\n",
      "epoch: 25 step: 45 acc: 0.97148436 loss: 0.34177813\n",
      "epoch: 25 step: 50 acc: 0.9097656 loss: 0.40349692\n",
      "epoch: 26 step: 5 acc: 0.94609374 loss: 0.36716878\n",
      "epoch: 26 step: 10 acc: 0.9507812 loss: 0.36248142\n",
      "epoch: 26 step: 15 acc: 0.94375 loss: 0.36951253\n",
      "epoch: 26 step: 20 acc: 0.95820314 loss: 0.3550602\n",
      "epoch: 26 step: 25 acc: 0.93125 loss: 0.382013\n",
      "epoch: 26 step: 30 acc: 0.9109375 loss: 0.402325\n",
      "epoch: 26 step: 35 acc: 0.88007814 loss: 0.43318444\n",
      "epoch: 26 step: 40 acc: 0.9207031 loss: 0.3925594\n",
      "epoch: 26 step: 45 acc: 0.95625 loss: 0.35701245\n",
      "epoch: 26 step: 50 acc: 0.93046874 loss: 0.38279378\n",
      "epoch: 27 step: 5 acc: 0.94257814 loss: 0.3706844\n",
      "epoch: 27 step: 10 acc: 0.953125 loss: 0.36013752\n",
      "epoch: 27 step: 15 acc: 0.95351565 loss: 0.35974687\n",
      "epoch: 27 step: 20 acc: 0.92890626 loss: 0.38435626\n",
      "epoch: 27 step: 25 acc: 0.92929685 loss: 0.38396564\n",
      "epoch: 27 step: 30 acc: 0.9171875 loss: 0.39607507\n",
      "epoch: 27 step: 35 acc: 0.8960937 loss: 0.4171688\n",
      "epoch: 27 step: 40 acc: 0.9015625 loss: 0.41170007\n",
      "epoch: 27 step: 45 acc: 0.86328125 loss: 0.4499812\n",
      "epoch: 27 step: 50 acc: 0.9203125 loss: 0.39295\n",
      "epoch: 28 step: 5 acc: 0.94921875 loss: 0.36404377\n",
      "epoch: 28 step: 10 acc: 0.9585937 loss: 0.35466877\n",
      "epoch: 28 step: 15 acc: 0.94023436 loss: 0.37302813\n",
      "epoch: 28 step: 20 acc: 0.9476563 loss: 0.3656063\n",
      "epoch: 28 step: 25 acc: 0.9285156 loss: 0.38474688\n",
      "epoch: 28 step: 30 acc: 0.9183594 loss: 0.39490312\n",
      "epoch: 28 step: 35 acc: 0.9105469 loss: 0.40271568\n",
      "epoch: 28 step: 40 acc: 0.8890625 loss: 0.42420006\n",
      "epoch: 28 step: 45 acc: 0.934375 loss: 0.3788875\n",
      "epoch: 28 step: 50 acc: 0.93046874 loss: 0.3827938\n",
      "epoch: 29 step: 5 acc: 0.92890626 loss: 0.38435626\n",
      "epoch: 29 step: 10 acc: 0.94804686 loss: 0.3652156\n",
      "epoch: 29 step: 15 acc: 0.91484374 loss: 0.3984188\n",
      "epoch: 29 step: 20 acc: 0.96367186 loss: 0.3495906\n",
      "epoch: 29 step: 25 acc: 0.95585936 loss: 0.35740316\n",
      "epoch: 29 step: 30 acc: 0.9152344 loss: 0.3980282\n",
      "epoch: 29 step: 35 acc: 0.9320313 loss: 0.38123125\n",
      "epoch: 29 step: 40 acc: 0.9566406 loss: 0.35662192\n",
      "epoch: 29 step: 45 acc: 0.9390625 loss: 0.37420002\n",
      "epoch: 29 step: 50 acc: 0.91445315 loss: 0.39880943\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" (if you want to use GPU2)\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "sentence_list_fname = './assignment_training_data_word_segment.json'\n",
    "sentence_list = json.load(open(sentence_list_fname , 'r'))\n",
    "\n",
    "# How to load vocabulary in Python:\n",
    "\n",
    "import pickle # (in python 3.x)\n",
    "# import cPickle as pickle # (in python 2.x)\n",
    "\n",
    "# voc_dict_fname = './voc-d.pkl'\n",
    "# voc_dict = pickle.load(open(voc_dict_fname, 'rb'))\n",
    "# idx2word, word2idx = voc_dict['idx2word'], voc_dict['word2idx'] \n",
    "# idx2word[index] is a word\n",
    "# word2idx[word] is an index\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def process(data):\n",
    "    result=[]\n",
    "    num =0\n",
    "    index = 0\n",
    "    for sentence in data:\n",
    "        t_pos = -1\n",
    "        t_num = len(sentence['times'])\n",
    "        a_num = len(sentence['attributes'])\n",
    "        v_num = len(sentence['values'])\n",
    "        sentence['testset']=[]\n",
    "        comma = [0]*5\n",
    "        semicolon = 0\n",
    "        comma_now = 0\n",
    "        for j in sentence['indexes']:\n",
    "            if j == 7 :\n",
    "                comma[comma_now] = j\n",
    "                comma_now +=1\n",
    "            if comma_now > 4:\n",
    "                break\n",
    "        for j in sentence['indexes']:\n",
    "            if j == 26 :\n",
    "                semicolon = j\n",
    "                break   \n",
    "        for t in sentence['times']:\n",
    "            t_pos += 1\n",
    "            a_pos = -1\n",
    "            for a in sentence['attributes']:\n",
    "                a_pos += 1\n",
    "                v_pos = -1\n",
    "                for v in sentence['values']:\n",
    "                    v_pos += 1\n",
    "                    origin_triple=[t,a,v]\n",
    "                    true=[0]*2\n",
    "                    true[0]=1\n",
    "                    for i in sentence['results']:\n",
    "                        if((i[0]==origin_triple[0]) and(i[1]==origin_triple[1]) and (i[2]==origin_triple[2])):\n",
    "                            true[1]=1\n",
    "                            true[0]=0\n",
    "                            break\n",
    "#                         print(num,v_num,v_pos)\n",
    "#                     w_info=[0]*(225+225+400+20)\n",
    "#                     w_info[t_num*15 + t_pos] = 1\n",
    "#                     w_info[225+a_num*15 + a_pos] = 1\n",
    "#                     w_info[450+v_num*20 + v_pos] = 1\n",
    "#                     w_info[851] = a-t\n",
    "#                     w_info[852] = v-a\n",
    "#                     w_info[853] = t_pos\n",
    "#                     w_info[854] = a_pos\n",
    "#                     w_info[855] = v_pos\n",
    "#                     w_info[856] = t_num\n",
    "#                     w_info[857] = a_num\n",
    "#                     w_info[858] = v_num\n",
    "#                     w_info[859] = t\n",
    "#                     w_info[860] = a\n",
    "#                     w_info[861] = v\n",
    "#                     w_info[862] = comma\n",
    "#                     w_info[863] = semicolon\n",
    "#                     w_info[675] = a-t\n",
    "#                     w_info[676] = v-a\n",
    "#                     w_info[677] = t_pos\n",
    "#                     w_info[678] = a_pos\n",
    "#                     w_info[679] = v_pos\n",
    "#                     w_info[680] = t_num\n",
    "#                     w_info[681] = a_num\n",
    "#                     w_info[682] = v_num\n",
    "#                     w_info[683] = t\n",
    "#                     w_info[684] = a\n",
    "#                     w_info[685] = v\n",
    "                    w_info=[0]*20\n",
    "                    w_info[0] = t_pos\n",
    "                    w_info[1] = a_pos\n",
    "                    w_info[2] = v_pos\n",
    "                    w_info[3] = t_num\n",
    "                    w_info[4] = a_num\n",
    "                    w_info[5] = v_num\n",
    "                    w_info[6] = t\n",
    "                    w_info[7] = a\n",
    "                    w_info[8] = v\n",
    "                    w_info[9] = comma[0]\n",
    "                    w_info[10] = comma[1]\n",
    "                    w_info[11] = comma[2]\n",
    "                    w_info[12] = comma[3]\n",
    "                    w_info[13] = comma[4]\n",
    "                    w_info[14] = semicolon\n",
    "                    w_info[15] = t-a\n",
    "                    w_info[16] = a-v\n",
    "                    w_info[17] = comma[0] - t\n",
    "                    w_info[18] = comma[0] - a\n",
    "                    w_info[19] = comma[1] - v\n",
    "                    sentence['testset'].append([true, w_info,origin_triple])\n",
    "                    result.append([true, w_info,origin_triple])\n",
    "                    num+=1\n",
    "#                     if num > 1000:\n",
    "#                         index =1\n",
    "#                         break\n",
    "#                 if index == 1:\n",
    "#                     break\n",
    "#             if index == 1:\n",
    "#                 break\n",
    "                \n",
    "    return result,num\n",
    "\n",
    "data,num = process(sentence_list)\n",
    "print(num)\n",
    "print(data[0],data[1],data[2])\n",
    "\n",
    "train_rate=0.001\n",
    "train_step=10000\n",
    "batch_size=2560\n",
    "\n",
    "\n",
    "frame_size=20\n",
    "# sequence_length=58\n",
    "sequence_length= 1\n",
    "hidden_num=128\n",
    "n_classes=2\n",
    "\n",
    "#定义输入,输出\n",
    "x=tf.placeholder(dtype=tf.float32,shape=[None,sequence_length*frame_size],name=\"x\")\n",
    "y=tf.placeholder(dtype=tf.float32,shape=[None,n_classes],name=\"y\")\n",
    "#定义权值\n",
    "weights=tf.Variable(tf.truncated_normal(shape=[hidden_num,n_classes]))\n",
    "bias=tf.Variable(tf.zeros(shape=[n_classes]))\n",
    "\n",
    "# def get_rnn_cell():\n",
    "#     return tf.nn.rnn_cell.BasicRNNCell(hidden_num)\n",
    "\n",
    "def RNN(x,weights,bias):\n",
    "    x=tf.reshape(x,shape=[-1,sequence_length,frame_size])\n",
    "    rnn_cell_mine=tf.nn.rnn_cell.BasicRNNCell(hidden_num)\n",
    "#     rnn_cell_mine=tf.nn.rnn_cell.MultiRNNCell([rnn_cell_mine] * 3)\n",
    "    init_state=tf.zeros(shape=[batch_size,rnn_cell_mine.state_size])\n",
    "    output,states=tf.nn.dynamic_rnn(rnn_cell_mine,x,dtype=tf.float32)\n",
    "#     print(output)\n",
    "    return tf.nn.softmax(tf.matmul(output[:,-1,:],weights)+bias,1)\n",
    "predy=RNN(x,weights,bias)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predy,labels=y))\n",
    "# print(predy)\n",
    "# pred = predy[0]\n",
    "# print(y)\n",
    "# y_ = y[0]\n",
    "# cost_=np.square(predy - y)\n",
    "# print(cost_)\n",
    "# cost=tf.reduce_mean(cost_)\n",
    "# # cost=tf.losses.mean_squared_error(cost_)\n",
    "train=tf.train.AdamOptimizer(train_rate).minimize(cost)\n",
    "\n",
    "correct_pred=tf.equal(tf.argmax(predy,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.to_float(correct_pred))\n",
    "\n",
    "\n",
    "def next_batch(num_in,total,current_pos):\n",
    "    result_x = []\n",
    "    result_y = []\n",
    "    for i in range(num_in):\n",
    "        sentence_current = data[current_pos] \n",
    "        result_x.append(sentence_current[1])\n",
    "        result_y.append(sentence_current[0])\n",
    "        current_pos+=1\n",
    "        if (current_pos ==  total -1):\n",
    "            current_pos = 0\n",
    "    result_y = np.reshape(result_y,[-1,2])\n",
    "#     print(current_pos)\n",
    "    return result_x,result_y,current_pos\n",
    "\n",
    "def test_next_batch(num):\n",
    "    result_x = []\n",
    "    result_y = []\n",
    "    for i in range(num):\n",
    "        sentence_current = data[i] \n",
    "        result_x.append(sentence_current[1])\n",
    "        result_y.append(sentence_current[0])\n",
    "    result_y = np.reshape(result_y,[-1,2])\n",
    "    return result_x,result_y\n",
    "\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "step=1\n",
    "test_x,test_y=test_next_batch(num)\n",
    "\n",
    "\n",
    "batch_num = num\n",
    "i=0\n",
    "epochs =30\n",
    "current_pos = 0\n",
    "for epoch in range(epochs):\n",
    "    # for i in range(batch_num):\n",
    "    # sentence_current = data[i]\n",
    "    # batch_x, batch_y = np.reshape(sentence_current[1],[-1,900]), np.reshape(sentence_current[0],[-1,1])\n",
    "    for step in range(50):\n",
    "        batch_x, batch_y,current_pos = next_batch(batch_size,batch_num,current_pos)\n",
    "    #       batch_x=tf.reshape(batch_x,shape=[batch_size,sequence_length,frame_size])\n",
    "        _loss,__=sess.run([cost,train],feed_dict={x:batch_x,y:batch_y})\n",
    "            \n",
    "        step+=1\n",
    "        if step  %5 ==0:\n",
    "            acc,loss=sess.run([accuracy,cost],feed_dict={x:batch_x,y:batch_y})\n",
    "            print(\"epoch:\",epoch,\"step:\",step,\"acc:\",acc,\"loss:\",loss)\n",
    "# #     _loss,__=sess.run([cost,train],feed_dict={x:test_x,y:test_y})\n",
    "# #     acc,loss=sess.run([accuracy,cost],feed_dict={x:test_x,y:test_y})\n",
    "#     print(\"!epoch:\",epoch,\"acc:\",acc,\"loss:\",loss)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
